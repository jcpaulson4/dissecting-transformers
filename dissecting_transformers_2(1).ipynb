{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uscJmrnissIQ"
      },
      "source": [
        "<a\n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab7.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGmg2HacVK97"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XXGqjrC-VK-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab731de-dbf6-41da-ca9e-8f7dadb99a37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opustools\n",
            "  Downloading opustools-1.6.1-py3-none-any.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ruamel.yaml (from opustools)\n",
            "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->opustools)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml.clib, ruamel.yaml, opustools\n",
            "Successfully installed opustools-1.6.1 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8\n",
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.17.1\n",
            "    Uninstalling torchtext-0.17.1:\n",
            "      Successfully uninstalled torchtext-0.17.1\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torchtext-0.6.0\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting fr-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.1.5)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Checking out Books...\n",
            "No alignment file \"/projappl/nlpl/data/OPUS/Books/latest/xml/en-fr.xml.gz\" or \"./Books_latest_xml_en-fr.xml.gz\" found\n",
            "The following files are available for downloading:\n",
            "\n",
            "   1 MB https://object.pouta.csc.fi/OPUS-Books/v1/xml/en-fr.xml.gz | 26 documents, 129305 alignment pairs, 3289328 source tokens, 3131177 target tokens (id 31468)\n",
            "  71 MB https://object.pouta.csc.fi/OPUS-Books/v1/xml/en.zip | 42 documents, 239555 sentences, 5864916 tokens (id 31665)\n",
            "  33 MB https://object.pouta.csc.fi/OPUS-Books/v1/xml/fr.zip | 29 documents, 164916 sentences, 3635088 tokens (id 31687)\n",
            "\n",
            " 105 MB Total size\n",
            "./Books_latest_xml_en-fr.xml.gz ... 100% of 1 MB\n",
            "./Books_latest_xml_en.zip ... 100% of 71 MB\n",
            "./Books_latest_xml_fr.zip ... 100% of 33 MB\n",
            "\u001b[2KParsing file \"./Books_latest_xml_en-fr.xml.gz\" ... 0.0%\n",
            "\u001b[2KParsing file \"Books/xml/en/Alain_Fournier-Le_grand_Meaulnes.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Alain_Fournier-Le_grand_Meaulnes.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\u001b[2KParsing file \"./Books_latest_xml_en-fr.xml.gz\" ... 6.01%\n",
            "\u001b[2KParsing file \"Books/xml/en/Austen_Jane-Pride_and_Prejudice.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Austen_Jane-Pride_and_Prejudice.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\u001b[2KParsing file \"./Books_latest_xml_en-fr.xml.gz\" ... 7.68%\n",
            "\u001b[2KParsing file \"Books/xml/en/Bronte_Charlotte-Jane_Eyre.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Bronte_Charlotte-Jane_Eyre.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\n",
            "\u001b[2KParsing file \"Books/xml/en/Carroll_Lewis-Alice_in_wonderland.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Carroll_Lewis-Alice_in_wonderland.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\n",
            "\u001b[2KParsing file \"Books/xml/en/Defoe_Daniel-Moll_Flanders.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Defoe_Daniel-Moll_Flanders.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\n",
            "\u001b[2KParsing file \"Books/xml/en/Defoe_Daniel-Robinson_Crusoe.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Defoe_Daniel-Robinson_Crusoe.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\n",
            "\u001b[2KParsing file \"Books/xml/en/Doyle_Arthur_Conan-A_Study_in_Scarlet.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Doyle_Arthur_Conan-A_Study_in_Scarlet.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\u001b[2KParsing file \"./Books_latest_xml_en-fr.xml.gz\" ... 19.71%\n",
            "\u001b[2KParsing file \"Books/xml/en/Doyle_Arthur_Conan-Great_Shadow.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Doyle_Arthur_Conan-Great_Shadow.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\u001b[2KParsing file \"./Books_latest_xml_en-fr.xml.gz\" ... 23.22%\n",
            "\u001b[2KParsing file \"Books/xml/en/Doyle_Arthur_Conan-Hound_of_the_Baskervilles.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Doyle_Arthur_Conan-Hound_of_the_Baskervilles.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\u001b[2KParsing file \"./Books_latest_xml_en-fr.xml.gz\" ... 25.89%\n",
            "\u001b[2KParsing file \"Books/xml/en/Doyle_Arthur_Conan-Rodney_Stone.xml\" ... 99.97%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Doyle_Arthur_Conan-Rodney_Stone.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\n",
            "\u001b[2KParsing file \"Books/xml/en/Dumas_Alexandre-Dame_aux_Camelias.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Dumas_Alexandre-Dame_aux_Camelias.xml\" ... 100.0%\n",
            "\n",
            "\u001b[2KParsing file \"Books/xml/en/Dumas_Alexandre-Trois_Mousquetaires.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Dumas_Alexandre-Trois_Mousquetaires.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\n",
            "\u001b[2KParsing file \"Books/xml/en/Flaubert_Gustave-Madame_Bovary.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Flaubert_Gustave-Madame_Bovary.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\u001b[2KParsing file \"./Books_latest_xml_en-fr.xml.gz\" ... 48.11%\n",
            "\u001b[2KParsing file \"Books/xml/en/Hugo_Victor-Notre_Dame_de_Paris.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Hugo_Victor-Notre_Dame_de_Paris.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\n",
            "\u001b[2KParsing file \"Books/xml/en/Jerome_Jerome_K-Three_Men_in_a_Boat.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Jerome_Jerome_K-Three_Men_in_a_Boat.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\n",
            "\u001b[2KParsing file \"Books/xml/en/Maupassant_Guy_de-Pierre_et_Jean.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Maupassant_Guy_de-Pierre_et_Jean.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\n",
            "\u001b[2KParsing file \"Books/xml/en/Poe_Edgar_Allan-Fall_of_the_House_of_Usher.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Poe_Edgar_Allan-Fall_of_the_House_of_Usher.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\n",
            "\u001b[2KParsing file \"Books/xml/en/Stendhal-Chartreuse_de_Parme.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Stendhal-Chartreuse_de_Parme.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\u001b[2KParsing file \"./Books_latest_xml_en-fr.xml.gz\" ... 61.48%\n",
            "\u001b[2KParsing file \"Books/xml/en/Stendhal-Rouge_et_le_Noir.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Stendhal-Rouge_et_le_Noir.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\n",
            "\u001b[2KParsing file \"Books/xml/en/Verne_Jules-20000_lieues_sous_les_mers.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Verne_Jules-20000_lieues_sous_les_mers.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\n",
            "\u001b[2KParsing file \"Books/xml/en/Verne_Jules-Ile_mysterieuse.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Verne_Jules-Ile_mysterieuse.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\n",
            "\u001b[2KParsing file \"Books/xml/en/Verne_Jules-Tour_du_monde_en_80_jours.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Verne_Jules-Tour_du_monde_en_80_jours.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\n",
            "\u001b[2KParsing file \"Books/xml/en/Verne_Jules-Voyage_au_Centre_de_la_Terre.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Verne_Jules-Voyage_au_Centre_de_la_Terre.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\n",
            "\u001b[2KParsing file \"Books/xml/en/Voltaire-Candide.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Voltaire-Candide.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\u001b[2KParsing file \"./Books_latest_xml_en-fr.xml.gz\" ... 91.55%\n",
            "\u001b[2KParsing file \"Books/xml/en/Zola_Emile-Germinal.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Zola_Emile-Germinal.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\n",
            "\u001b[2KParsing file \"Books/xml/en/Zola_Emile-Therese_Raquin.xml\" ... 100.0%\n",
            "\u001b[2KParsing file \"Books/xml/fr/Zola_Emile-Therese_Raquin.xml\" ... 100.0%\n",
            "\u001b[F\u001b[F\u001b[F\u001b[2KParsing file \"./Books_latest_xml_en-fr.xml.gz\" ... 100.0%\n",
            "\n",
            "\n",
            "\n",
            "...collating samples...\n",
            "...127085 samples processed!\n",
            "Splitting data into test/dev/train sets...\n",
            "...done!\n",
            "Writing test data to `test.{en,fr,ids}'...\n",
            "...done!\n",
            "Writing development data to `dev.{en,fr,ids}'...\n",
            "...done!\n",
            "Writing training data to `train.{en,fr,ids}'...\n",
            "...done!\n"
          ]
        }
      ],
      "source": [
        "! pip install opustools\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import opustools\n",
        "from torchmetrics.text import CHRFScore\n",
        "seaborn.set_context(context=\"talk\")\n",
        "%matplotlib inline\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# Download english and spanish vocab and general conference texts\n",
        "!pip install torchtext==0.6.0\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm\n",
        "# !wget  -O ./spanish \"https://raw.githubusercontent.com/nickwalton/translation/master/gc_2010-2017_conglomerated_20171009_es.txt\"\n",
        "# !wget -O ./english \"https://raw.githubusercontent.com/nickwalton/translation/master/gc_2010-2017_conglomerated_20171009_en.txt\"\n",
        "!cd french\n",
        "!opus_express -s en -t fr -c Books -q\n",
        "!cd ../finnish\n",
        "!opus_express -s en -t fi -c Books -q\n",
        "!cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPjcbr94CPVY"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt7NUbJMVK-H"
      },
      "source": [
        "## Model Helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0ehG_01DVK-J"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Simple linear layers with dropout and relu\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"Create word embeddings\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module \"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn237Niw3v5R"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "The encoder is composed of a stack of $N=6$ identical layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w0jPCmNjpzqQ"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward \"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "        self.visualizing = False\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask, viz=self.visualizing))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwl4OmC8VK-n"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "The decoder is also composed of a stack of $N=6$ identical layers.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tVNegfOCVK-o"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "\n",
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJlAhkFRVK-5"
      },
      "source": [
        "## Implement Attention\n",
        "\n",
        "\n",
        "https://arxiv.org/pdf/1706.03762.pdf         \n",
        "                                                                                                                                                                     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D_5M7GA9VK-_"
      },
      "outputs": [],
      "source": [
        "ALL_SCORES = []\n",
        "\n",
        "def attention(query, key, value, mask, viz=False, h=-1):\n",
        "    # Compute 'Scaled Dot Product Attention'\n",
        "\n",
        "    # scores = QK^T/scale\n",
        "    scores = (torch.bmm(query, key.permute(0, 2, 1))) / math.sqrt(query.size(-1))\n",
        "\n",
        "    if viz:\n",
        "        ALL_SCORES.append((h, scores.cpu().detach().numpy()))\n",
        "\n",
        "    # Apply the mask\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    # output = softmax(scores)(V)\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    output = torch.bmm(attention_weights, value)\n",
        "\n",
        "    return output\n",
        "\n",
        "# query = torch.empty(33, 26, 256).uniform_(-1, 1)\n",
        "# key = torch.empty(33, 26, 256).uniform_(-1, 1)\n",
        "# value = torch.empty(33, 26, 256).uniform_(-1, 1)\n",
        "# mask = torch.empty(33, 1, 26).uniform_(-1, 1)\n",
        "\n",
        "# a = attention(query, key, value, mask)\n",
        "# print(a.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Mv1rWttbVK_K"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1, d_v=None):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        # Implement Multi-head attention mechanism\n",
        "        assert d_model % h == 0\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.d_v = d_v\n",
        "        if d_v is None:\n",
        "            self.d_v = self.d_k\n",
        "\n",
        "        # Make an attention head (linear layers for q, k, and v)\n",
        "        # Make h copies of the attention head (Hint: See the `clone()` helper function)\n",
        "        # (turns out \"clone\" doesn't work on nn.Linear)\n",
        "        self.heads = nn.ModuleList([\n",
        "            nn.ModuleList([\n",
        "                nn.Linear(d_model, self.d_k),\n",
        "                nn.Linear(d_model, self.d_k),\n",
        "                nn.Linear(d_model, self.d_k)\n",
        "            ]) for _ in range(h)\n",
        "        ])\n",
        "        self.linear_out = nn.Linear(h * self.d_v, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask, viz=False):\n",
        "        # For each attention head\n",
        "            # Calculate the portion of the query, key, value that will be passed into the head\n",
        "            # Pass the query, key, value through their respective layers\n",
        "            # Compute attention using attention(query, value, key, mask)\n",
        "            # attention() outputs\n",
        "        batch_size, seq_length, _ = query.shape\n",
        "        results = []\n",
        "        for head, (WQ, WK, WV) in enumerate(self.heads):\n",
        "            Q, K, V = WQ(query), WK(key), WV(value)\n",
        "            a = attention(Q, K, V, mask, viz, head)\n",
        "            results.append(a)\n",
        "        output = torch.cat(results, dim=2)\n",
        "        return self.linear_out(output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32CAx69kVK_V"
      },
      "source": [
        "## Positional Encoding                                                                                                                             \n",
        "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.  To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.  The positional encodings have the same dimension $d_{\\text{model}}$ as the embeddings, so that the two can be summed.   There are many choices of positional encodings, learned and fixed [(cite)](https://arxiv.org/pdf/1705.03122.pdf).\n",
        "\n",
        "In this work, we use sine and cosine functions of different frequencies:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
        "$$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}})$$\n",
        "\n",
        "$$PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})$$                                                                                                                                                                                                                                                        \n",
        "where $pos$ is the position and $i$ is the dimension.  That is, each dimension of the positional encoding corresponds to a sinusoid.  The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi$.  We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of $PE_{pos}$.\n",
        "\n",
        "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.  For the base model, we use a rate of $P_{drop}=0.1$.\n",
        "                                                                                                                                                                                                                                                    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MZdSqDxuVK_V"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = 1 / (10000 ** (torch.arange(0., d_model, 2) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)],\n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ5w5s1vVK_f"
      },
      "source": [
        "## Full Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oA5SHpQpVK_f"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Full transformer model\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab, tgt_vocab, N=6, d_model=256, d_ff=1024, h=4, dropout=0.1, dec=1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        attn = MultiHeadedAttention(h, d_model)\n",
        "        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        position = PositionalEncoding(d_model, dropout)\n",
        "        c = copy.deepcopy\n",
        "\n",
        "        self.encoder = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N)\n",
        "\n",
        "        self.decoders = [Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N) for _ in range(dec)]\n",
        "        self.src_embeds = [nn.Sequential(Embeddings(d_model, src_vocab[i]), c(position)) for i in range(dec)]\n",
        "        self.tgt_embeds = [nn.Sequential(Embeddings(d_model, tgt_vocab[i]), c(position)) for i in range(dec)]\n",
        "        self.generators = [Generator(d_model, tgt_vocab[i]) for i in range(dec)]\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "        self.index = 1\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask,\n",
        "                            tgt, tgt_mask)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoders[self.index](self.src_embeds[self.index](src), src_mask)\n",
        "\n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoders[self.index](self.tgt_embeds[self.index](tgt), memory, src_mask, tgt_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9aZ53S-VK_k"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM7uHa_0VK_m"
      },
      "source": [
        "## Batches and Masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b0gFTX4gVK_n"
      },
      "outputs": [],
      "source": [
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, trg=None, pad=0):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if trg is not None:\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = \\\n",
        "                self.make_std_mask(self.trg, pad)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum()\n",
        "\n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words.\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & Variable(\n",
        "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        return tgt_mask\n",
        "\n",
        "\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQMYhWxKVK_9"
      },
      "source": [
        "## Label Smoothing\n",
        "\n",
        "During training, we employed label smoothing of value $\\epsilon_{ls}=0.1$ [(cite)](https://arxiv.org/abs/1512.00567).  This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "H7My_MENVK_-"
      },
      "outputs": [],
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1e4lIz1VLAT"
      },
      "source": [
        "## Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk6oMyrxvMXk",
        "outputId": "e481ba4c-cc6e-4b3b-a238-41b2293bdc96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Dataset\n"
          ]
        }
      ],
      "source": [
        "from torchtext import data, datasets\n",
        "import torchtext\n",
        "import spacy\n",
        "\n",
        "# Load spacy tokenizers.\n",
        "# spacy_es = spacy.load('es_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "spacy_fr = spacy.load('fr_core_news_sm')\n",
        "\n",
        "# def tokenize_es(text):\n",
        "#     return [tok.text for tok in spacy_es.tokenizer(text)]\n",
        "\n",
        "def tokenize_fr(text):\n",
        "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'\n",
        "BLANK_WORD = \"<blank>\"\n",
        "SRC1 = data.Field(tokenize=tokenize_en, pad_token=BLANK_WORD)\n",
        "TGT1 = data.Field(tokenize=tokenize_fr, init_token = BOS_WORD,\n",
        "                 eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
        "SRC2 = data.Field(tokenize=tokenize_en, pad_token=BLANK_WORD)\n",
        "TGT2 = data.Field(tokenize=tokenize_fr, init_token = BOS_WORD,\n",
        "                 eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
        "\n",
        "print(\"Loading Dataset\")\n",
        "with open('./french/train.en', 'r', encoding='utf-8') as english_text:\n",
        "    english_lines1 = list(english_text)\n",
        "with open('./french/train.fr', 'r', encoding='utf-8') as french_text:\n",
        "    french_lines1 = list(french_text)\n",
        "with open('./finnish/train.en', 'r', encoding='utf-8') as english_text:\n",
        "    english_lines2 = list(english_text)\n",
        "with open('./finnish/train.fr', 'r', encoding='utf-8') as finnish_text:\n",
        "    finnish_lines2 = list(finnish_text)\n",
        "\n",
        "fields = ([\"src\", SRC], [\"trg\", TGT])\n",
        "examples1 = [data.Example.fromlist((english_lines1[i], french_lines1[i]), fields ) for i in range(len(english_lines1))]\n",
        "examples2 = [data.Example.fromlist((english_lines2[i], finnish_lines2[i]), fields ) for i in range(len(english_lines2))]\n",
        "\n",
        "MAX_LEN = 200\n",
        "train1, val1 = data.Dataset(examples1, fields=fields, filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and\n",
        "        len(vars(x)['trg']) <= MAX_LEN).split()\n",
        "train2, val2 = data.Dataset(examples1, fields=fields, filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and\n",
        "        len(vars(x)['trg']) <= MAX_LEN).split()\n",
        "\n",
        "MIN_FREQ = 1\n",
        "SRC1.build_vocab(train1.src, min_freq=MIN_FREQ)\n",
        "TGT1.build_vocab(train1.trg, min_freq=MIN_FREQ)\n",
        "SRC2.build_vocab(train2.src, min_freq=MIN_FREQ)\n",
        "TGT2.build_vocab(train2.trg, min_freq=MIN_FREQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKFl6R5NVLAi"
      },
      "source": [
        "## Training Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5aSC2-z9Gx6Y"
      },
      "outputs": [],
      "source": [
        "class LossFunction:\n",
        "    \"A simple loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "\n",
        "    def __call__(self, x, y, norm):\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
        "                              y.contiguous().view(-1)) / norm\n",
        "        loss.backward()\n",
        "        if self.opt is not None:\n",
        "            self.opt.step()\n",
        "            self.opt.zero_grad()\n",
        "        return loss.data * norm\n",
        "\n",
        "class DataIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "\n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"Fix order in torchtext to match ours\"\n",
        "    src, trg = batch.src.transpose(0, 1).to(device), batch.trg.transpose(0, 1).to(device)\n",
        "    return Batch(src, trg, pad_idx)\n",
        "\n",
        "\n",
        "def run_epoch(data_iter1, dataiter2, model, loss_compute):\n",
        "    \"Standard Training and Logging Function\"\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    for i, batches in enumerate(zip(data_iter1, data_iter2)):\n",
        "        for j in range(2):\n",
        "            model.index = j+1\n",
        "            batch = batches[j]\n",
        "\n",
        "            out = model.forward(batch.src, batch.trg,\n",
        "                                batch.src_mask, batch.trg_mask)\n",
        "            loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
        "            total_loss += loss\n",
        "            total_tokens += batch.ntokens\n",
        "            tokens += batch.ntokens\n",
        "            if i % 50 == 1:\n",
        "                elapsed = time.time() - start\n",
        "                print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                        (i, loss / batch.ntokens, tokens / elapsed))\n",
        "                start = time.time()\n",
        "                tokens = 0\n",
        "    return total_loss / total_tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T97ls8JtZ_6C"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejIuRQylVLAm"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "pad_idx1 = TGT1.vocab.stoi[\"<blank>\"]\n",
        "pad_idx2 = TGT2.vocab.stoi[\"<blank>\"]\n",
        "model = TransformerModel(len(SRC1.vocab), len(TGT1.vocab), N=2, dec=2, decvs=len(SRC2.vocab), decvt=len(TGT2.vocab)).to(device)\n",
        "n_epochs = 3\n",
        "\n",
        "def scope():\n",
        "    criterion1 = LabelSmoothing(size=len(TGT1.vocab), padding_idx=pad_idx1, smoothing=0.1).to(device)\n",
        "    criterion2 = LabelSmoothing(size=len(TGT2.vocab), padding_idx=pad_idx2, smoothing=0.1).to(device)\n",
        "    BATCH_SIZE = 1000\n",
        "    train_iter1 = DataIterator(train1, batch_size=BATCH_SIZE, device=device,\n",
        "                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                            batch_size_fn=batch_size_fn, train=True)\n",
        "    train_iter2 = DataIterator(train2, batch_size=BATCH_SIZE, device=device,\n",
        "                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                            batch_size_fn=batch_size_fn, train=True)\n",
        "\n",
        "    model_opt = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        run_epoch(((rebatch(pad_idx1, b1), rebatch(pad_idx2, b2)) for b1, b2 in zip(train_iter1, train_iter2)),\n",
        "                  model,\n",
        "                  LossFunction(model.generator, criterion, model_opt))\n",
        "        model.eval()\n",
        "scope()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgZTmtfBVLAo"
      },
      "source": [
        "## Translate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask,\n",
        "                           Variable(ys),\n",
        "                           Variable(subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    return ys"
      ],
      "metadata": {
        "id": "M_dfdP0D31bi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prRVRjYOVLAo"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 1000\n",
        "n_train_iters = len(train) / BATCH_SIZE\n",
        "valid_iter = DataIterator(val, batch_size=BATCH_SIZE, device=device,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=False)\n",
        "\n",
        "for outside_i, batch in enumerate(valid_iter):\n",
        "    src = batch.src.transpose(0, 1)[:1].to(device)\n",
        "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2).to(device)\n",
        "    out = greedy_decode(model, src, src_mask,\n",
        "                        max_len=60, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
        "    print(\"Original:\", end=\"\\t\")\n",
        "    for i in range(0, src.size(1)):\n",
        "        sym = SRC.vocab.itos[src[0, i]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    print(\"Translation:\", end=\"\\t\")\n",
        "    for i in range(1, out.size(1)):\n",
        "        sym = TGT.vocab.itos[out[0, i]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    print(\"Target:\\t\", end=\"\\t\")\n",
        "    for i in range(1, batch.trg.size(0)):\n",
        "        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    if outside_i > 40 and outside_i < 1100:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChrF"
      ],
      "metadata": {
        "id": "gakby3_Gz_bp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fexgduj70EjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dissection"
      ],
      "metadata": {
        "id": "gNAqKgJgck6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"./saved-model\")"
      ],
      "metadata": {
        "id": "TpL2LqtHchF7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(\"./saved-model\")"
      ],
      "metadata": {
        "id": "dbM3tALufCEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.encoder.layers[0].visualizing = True\n",
        "model.encoder.layers[1].visualizing = True\n",
        "\n",
        "sentences = (\n",
        "    \"He had three children .\",\n",
        "    \"He will have three friends .\",\n",
        "    \"His friends will have two parties .\",\n",
        "    \"Her name was on the list of people at the ball .\"\n",
        ")\n",
        "\n",
        "for raw_sentence in sentences:\n",
        "\n",
        "  ALL_SCORES = []\n",
        "  sentence = raw_sentence.split(\" \")\n",
        "  src1 = torch.tensor([SRC1.vocab.stoi[word] for word in sentence]).unsqueeze(-2).to(device)\n",
        "  src_mask1 = (src1 != SRC1.vocab.stoi[\"<blank>\"]).unsqueeze(-2).to(device)\n",
        "  model.index = 1\n",
        "  out1 = greedy_decode(model, src1, src_mask1,\n",
        "                          max_len=60, start_symbol=TGT1.vocab.stoi[\"<s>\"])\n",
        "  src2 = torch.tensor([SRC2.vocab.stoi[word] for word in sentence]).unsqueeze(-2).to(device)\n",
        "  src_mask2 = (src2 != SRC2.vocab.stoi[\"<blank>\"]).unsqueeze(-2).to(device)\n",
        "  model.index = 2\n",
        "  out2 = greedy_decode(model, src2, src_mask2,\n",
        "                          max_len=60, start_symbol=TGT2.vocab.stoi[\"<s>\"])\n",
        "\n",
        "  print(\"Original:\", end=\"\\t\\t\")\n",
        "  for i in range(0, src1.size(1)):\n",
        "      sym = SRC.vocab.itos[src1[0, i]]\n",
        "      if sym == \"</s>\": break\n",
        "      print(sym, end =\" \")\n",
        "  print(\"\\nFrench:\", end=\"\\t\")\n",
        "  for i in range(1, out1.size(1)):\n",
        "      sym = TGT1.vocab.itos[out1[0, i]]\n",
        "      if sym == \"</s>\": break\n",
        "      print(sym, end =\" \")\n",
        "  print(\"\\nFinnish:\", end=\"\\t\")\n",
        "  for i in range(1, out2.size(1)):\n",
        "      sym = TGT2.vocab.itos[out2[0, i]]\n",
        "      if sym == \"</s>\": break\n",
        "      print(sym, end =\" \")\n",
        "  print()\n",
        "\n",
        "  fig = plt.figure(figsize=(10,5))\n",
        "  for i, (h, ten) in enumerate(ALL_SCORES):\n",
        "      plt.subplot(2, 4, i+1)\n",
        "      # plt.axis('off')\n",
        "      plt.imshow(ten[0], cmap='viridis')\n",
        "      plt.xticks(range(len(sentence)), sentence, size='10', rotation=90)\n",
        "      plt.yticks(range(len(sentence)), sentence, size='10', rotation=0)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  plt.savefig(\"test.svg\")"
      ],
      "metadata": {
        "id": "FQQVd7AMPuqo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tPjcbr94CPVY",
        "nM7uHa_0VK_m",
        "sQMYhWxKVK_9"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}